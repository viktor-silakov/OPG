{"created": 1750691542.5071301, "duration": 368.23134422302246, "exitcode": 0, "root": "/Users/a1/Project/OPG/fs-python/tests", "environment": {}, "summary": {"passed": 1, "total": 1, "collected": 1}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "test_e2e_training.py", "type": "Module"}]}, {"nodeid": "test_e2e_training.py::TestConfig", "outcome": "passed", "result": []}, {"nodeid": "test_e2e_training.py::TestE2ETraining", "outcome": "passed", "result": [{"nodeid": "test_e2e_training.py::TestE2ETraining::test_full_e2e_workflow", "type": "Function", "lineno": 506}]}, {"nodeid": "test_e2e_training.py", "outcome": "passed", "result": [{"nodeid": "test_e2e_training.py::TestConfig", "type": "Class"}, {"nodeid": "test_e2e_training.py::TestE2ETraining", "type": "Class"}]}], "tests": [{"nodeid": "test_e2e_training.py::TestE2ETraining::test_full_e2e_workflow", "lineno": 506, "outcome": "passed", "keywords": ["test_full_e2e_workflow", "e2e", "slow", "TestE2ETraining", "test_e2e_training.py", "tests"], "setup": {"duration": 0.00012583300122059882, "outcome": "passed"}, "call": {"duration": 367.56829741600086, "outcome": "passed", "stdout": "\ud83d\udc1f Starting Fish Speech E2E Training Test\n==================================================\n\ud83d\udd27 Setting up real Russian voice data...\n\u2705 Copied voice: RU_Google_Female_Zephyr (npy: 32KB)\n\u2705 Copied voice: RU_Male_Goblin_Puchkov (npy: 80KB)\n\u2705 Copied voice: RU_Google_Male_Achird (npy: 34KB)\n\u2705 Real Russian voice data prepared: 3/3 voices\n\ud83d\udd0d Running system checks...\n\ud83d\udd0d Checking system requirements\n\u2705 \u2705 MPS (Apple Silicon) is available\n\u2705 \u2705 All requirements are met\n\u2705 System checks passed\n\ud83c\udfad Verifying emotional tokens...\n\u2705 All 5 emotional tokens verified\n\ud83d\ude80 Starting initial training: e2e_test_initial\n\ud83d\udcc1 Using prepared semantic tokens directly: /Users/a1/Project/OPG/fs-python/tests/data/prepared\n\u2705 Prepared voice: RU_Google_Female_Zephyr\n\u2705 Prepared voice: RU_Male_Goblin_Puchkov\n\u2705 Prepared voice: RU_Google_Male_Achird\n\u2705 Prepared 3/3 voices with semantic tokens\n\ud83d\ude80 Starting fine-tuning project: e2e_test_initial\n\ud83d\udcca \ud83d\udcca Estimated training samples: ~164\n\ud83d\udcc8 \ud83d\udcc8 Steps per epoch: 164\n\ud83d\udcc8 \ud83d\udcc8 Max steps: 5\n\u26a0\ufe0f \u26a0\ufe0f No training samples found in dataset\n\ud83c\udd95 \ud83c\udd95 Starting from base model: /Users/a1/.cache/huggingface/hub/models--fishaudio--fish-speech-1.5/snapshots/275a984d33c33659e39eed41ff5bcd6e67517f4c\n\ud83d\udd27 \ud83d\udd27 Using optimized settings for memory economy:\n\ud83c\udd95    \u2022 Mode: New training from base model\n\ud83d\udcbb    \u2022 Device: mps\n\ud83d\udcca    \u2022 Batch size: 1\n\ud83d\udc77    \u2022 Data workers: 0\n\ud83d\udccf    \u2022 Maximum length: 512 tokens\n\ud83d\udcc8    \u2022 Maximum steps: 5\n\ud83c\udfaf    \u2022 Learning rate: 0.0001\n\ud83d\udd27    \u2022 LoRA config: r_8_alpha_16\n\ud83d\udca1 \ud83d\udca1 All our BFloat16\u2192Float32 conversions are already applied in the code\n\ud83d\udda5\ufe0f Training command:\n/Users/a1/Project/OPG/fs-python/.venv/bin/python fish_speech/train.py --config-name text2semantic_finetune project=e2e_test_initial +lora@model.model.lora_config=r_8_alpha_16 data.batch_size=1 model.optimizer.lr=0.0001 trainer.accelerator=mps trainer.devices=1 trainer.strategy=auto data.num_workers=0 paths.run_dir=/Users/a1/Project/OPG/checkpoints/e2e_test_initial paths.ckpt_dir=/Users/a1/Project/OPG/checkpoints/e2e_test_initial/checkpoints data.max_length=512 pretrained_ckpt_path=/Users/a1/.cache/huggingface/hub/models--fishaudio--fish-speech-1.5/snapshots/275a984d33c33659e39eed41ff5bcd6e67517f4c trainer.val_check_interval=3 trainer.max_steps=5 callbacks.learning_rate_monitor.logging_interval=step\n\n\u25b6\ufe0f \u25b6\ufe0f Starting training...\n\u26a0\ufe0f \u26a0\ufe0f If process is killed (-9), try reducing batch_size\n\u26a0\ufe0f Press Ctrl+C to stop\n[2025-06-23 19:06:33,465][__main__][INFO] - [rank: 0] Starting training!\nValidation DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07<00:00,  1.30it/s]\u001b[A\nEpoch 0:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13<00:08,  0.22it/s, v_num=0, train/loss=8.570, train/top_5_accuracy=0.371, val/loss=8.720, val/top_5_accuracy=0.375]\nEpoch 0:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14<00:03,  0.27it/s, v_num=0, train/loss=8.570, train/top_5_accuracy=0.371, val/loss=8.720, val/top_5_accuracy=0.375]\nEpoch 0:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14<00:03,  0.27it/s, v_num=0, train/loss=9.190, train/top_5_accuracy=0.322, val/loss=8.720, val/top_5_accuracy=0.375]\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:16<00:00,  0.31it/s, v_num=0, train/loss=9.190, train/top_5_accuracy=0.322, val/loss=8.720, val/top_5_accuracy=0.375]\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:16<00:00,  0.31it/s, v_num=0, train/loss=8.920, train/top_5_accuracy=0.363, val/loss=8.720, val/top_5_accuracy=0.375]\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:16<00:00,  0.31it/s, v_num=0, train/loss=8.920, train/top_5_accuracy=0.363, val/loss=8.720, val/top_5_accuracy=0.375]`Trainer.fit` stopped: `max_steps=5` reached.\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:16<00:00,  0.31it/s, v_num=0, train/loss=8.920, train/top_5_accuracy=0.363, val/loss=8.720, val/top_5_accuracy=0.375]\n\u2705 \u2705 Training completed successfully!\n\ud83d\udcc1 \ud83d\udcc1 Checkpoints saved in: /Users/a1/Project/OPG/checkpoints/e2e_test_initial/checkpoints\n\ud83d\udcc4    \ud83d\udcc4 step_000000003.ckpt\n\u2705 Initial training completed, checkpoint: step_000000003.ckpt\n\ud83d\udd04 Starting resume training: e2e_test_resume\n\ud83d\udd04 Resuming fine-tuning project: e2e_test_resume from checkpoint\n\ud83d\udcca \ud83d\udcca Estimated training samples: ~164\n\ud83d\udcc8 \ud83d\udcc8 Steps per epoch: 164\n\ud83d\udcc8 \ud83d\udcc8 Max steps: 8\n\u26a0\ufe0f \u26a0\ufe0f No training samples found in dataset\n\ud83d\udcc2 \ud83d\udcc2 Resuming from: /Users/a1/Project/OPG/checkpoints/e2e_test_initial/checkpoints/step_000000003.ckpt\n\ud83d\udd04 \ud83d\udd04 Resuming from checkpoint: /Users/a1/Project/OPG/checkpoints/e2e_test_initial/checkpoints/step_000000003.ckpt\n\ud83d\udd27 \ud83d\udd27 Using optimized settings for memory economy:\n\ud83d\udd04    \u2022 Mode: Resume training from checkpoint\n\ud83d\udcbb    \u2022 Device: mps\n\ud83d\udcca    \u2022 Batch size: 1\n\ud83d\udc77    \u2022 Data workers: 0\n\ud83d\udccf    \u2022 Maximum length: 512 tokens\n\ud83d\udcc8    \u2022 Maximum steps: 8\n\ud83c\udfaf    \u2022 Learning rate: 0.0001\n\ud83d\udd27    \u2022 LoRA config: r_8_alpha_16\n\ud83d\udca1 \ud83d\udca1 All our BFloat16\u2192Float32 conversions are already applied in the code\n\ud83d\udda5\ufe0f Training command:\n/Users/a1/Project/OPG/fs-python/.venv/bin/python fish_speech/train.py --config-name text2semantic_finetune project=e2e_test_resume +lora@model.model.lora_config=r_8_alpha_16 data.batch_size=1 model.optimizer.lr=0.0001 trainer.accelerator=mps trainer.devices=1 trainer.strategy=auto data.num_workers=0 paths.run_dir=/Users/a1/Project/OPG/checkpoints/e2e_test_resume paths.ckpt_dir=/Users/a1/Project/OPG/checkpoints/e2e_test_resume/checkpoints data.max_length=512 +ckpt_path=/Users/a1/Project/OPG/checkpoints/e2e_test_initial/checkpoints/step_000000003.ckpt +resume_weights_only=true trainer.val_check_interval=3 trainer.max_steps=8 callbacks.learning_rate_monitor.logging_interval=step\n\n\u25b6\ufe0f \u25b6\ufe0f Starting training...\n\u26a0\ufe0f \u26a0\ufe0f If process is killed (-9), try reducing batch_size\n\u26a0\ufe0f Press Ctrl+C to stop\n[2025-06-23 19:07:15,573][__main__][INFO] - [rank: 0] Starting training!\n[2025-06-23 19:07:15,617][__main__][INFO] - [rank: 0] Error loading state dict: _IncompatibleKeys(missing_keys=['model.embeddings.weight', 'model.codebook_embeddings.weight', 'model.layers.0.attention.wqkv.weight', 'model.layers.0.attention.wo.weight', 'model.layers.0.feed_forward.w1.weight', 'model.layers.0.feed_forward.w3.weight', 'model.layers.0.feed_forward.w2.weight', 'model.layers.0.ffn_norm.weight', 'model.layers.0.attention_norm.weight', 'model.layers.1.attention.wqkv.weight', 'model.layers.1.attention.wo.weight', 'model.layers.1.feed_forward.w1.weight', 'model.layers.1.feed_forward.w3.weight', 'model.layers.1.feed_forward.w2.weight', 'model.layers.1.ffn_norm.weight', 'model.layers.1.attention_norm.weight', 'model.layers.2.attention.wqkv.weight', 'model.layers.2.attention.wo.weight', 'model.layers.2.feed_forward.w1.weight', 'model.layers.2.feed_forward.w3.weight', 'model.layers.2.feed_forward.w2.weight', 'model.layers.2.ffn_norm.weight', 'model.layers.2.attention_norm.weight', 'model.layers.3.attention.wqkv.weight', 'model.layers.3.attention.wo.weight', 'model.layers.3.feed_forward.w1.weight', 'model.layers.3.feed_forward.w3.weight', 'model.layers.3.feed_forward.w2.weight', 'model.layers.3.ffn_norm.weight', 'model.layers.3.attention_norm.weight', 'model.layers.4.attention.wqkv.weight', 'model.layers.4.attention.wo.weight', 'model.layers.4.feed_forward.w1.weight', 'model.layers.4.feed_forward.w3.weight', 'model.layers.4.feed_forward.w2.weight', 'model.layers.4.ffn_norm.weight', 'model.layers.4.attention_norm.weight', 'model.layers.5.attention.wqkv.weight', 'model.layers.5.attention.wo.weight', 'model.layers.5.feed_forward.w1.weight', 'model.layers.5.feed_forward.w3.weight', 'model.layers.5.feed_forward.w2.weight', 'model.layers.5.ffn_norm.weight', 'model.layers.5.attention_norm.weight', 'model.layers.6.attention.wqkv.weight', 'model.layers.6.attention.wo.weight', 'model.layers.6.feed_forward.w1.weight', 'model.layers.6.feed_forward.w3.weight', 'model.layers.6.feed_forward.w2.weight', 'model.layers.6.ffn_norm.weight', 'model.layers.6.attention_norm.weight', 'model.layers.7.attention.wqkv.weight', 'model.layers.7.attention.wo.weight', 'model.layers.7.feed_forward.w1.weight', 'model.layers.7.feed_forward.w3.weight', 'model.layers.7.feed_forward.w2.weight', 'model.layers.7.ffn_norm.weight', 'model.layers.7.attention_norm.weight', 'model.layers.8.attention.wqkv.weight', 'model.layers.8.attention.wo.weight', 'model.layers.8.feed_forward.w1.weight', 'model.layers.8.feed_forward.w3.weight', 'model.layers.8.feed_forward.w2.weight', 'model.layers.8.ffn_norm.weight', 'model.layers.8.attention_norm.weight', 'model.layers.9.attention.wqkv.weight', 'model.layers.9.attention.wo.weight', 'model.layers.9.feed_forward.w1.weight', 'model.layers.9.feed_forward.w3.weight', 'model.layers.9.feed_forward.w2.weight', 'model.layers.9.ffn_norm.weight', 'model.layers.9.attention_norm.weight', 'model.layers.10.attention.wqkv.weight', 'model.layers.10.attention.wo.weight', 'model.layers.10.feed_forward.w1.weight', 'model.layers.10.feed_forward.w3.weight', 'model.layers.10.feed_forward.w2.weight', 'model.layers.10.ffn_norm.weight', 'model.layers.10.attention_norm.weight', 'model.layers.11.attention.wqkv.weight', 'model.layers.11.attention.wo.weight', 'model.layers.11.feed_forward.w1.weight', 'model.layers.11.feed_forward.w3.weight', 'model.layers.11.feed_forward.w2.weight', 'model.layers.11.ffn_norm.weight', 'model.layers.11.attention_norm.weight', 'model.layers.12.attention.wqkv.weight', 'model.layers.12.attention.wo.weight', 'model.layers.12.feed_forward.w1.weight', 'model.layers.12.feed_forward.w3.weight', 'model.layers.12.feed_forward.w2.weight', 'model.layers.12.ffn_norm.weight', 'model.layers.12.attention_norm.weight', 'model.layers.13.attention.wqkv.weight', 'model.layers.13.attention.wo.weight', 'model.layers.13.feed_forward.w1.weight', 'model.layers.13.feed_forward.w3.weight', 'model.layers.13.feed_forward.w2.weight', 'model.layers.13.ffn_norm.weight', 'model.layers.13.attention_norm.weight', 'model.layers.14.attention.wqkv.weight', 'model.layers.14.attention.wo.weight', 'model.layers.14.feed_forward.w1.weight', 'model.layers.14.feed_forward.w3.weight', 'model.layers.14.feed_forward.w2.weight', 'model.layers.14.ffn_norm.weight', 'model.layers.14.attention_norm.weight', 'model.layers.15.attention.wqkv.weight', 'model.layers.15.attention.wo.weight', 'model.layers.15.feed_forward.w1.weight', 'model.layers.15.feed_forward.w3.weight', 'model.layers.15.feed_forward.w2.weight', 'model.layers.15.ffn_norm.weight', 'model.layers.15.attention_norm.weight', 'model.layers.16.attention.wqkv.weight', 'model.layers.16.attention.wo.weight', 'model.layers.16.feed_forward.w1.weight', 'model.layers.16.feed_forward.w3.weight', 'model.layers.16.feed_forward.w2.weight', 'model.layers.16.ffn_norm.weight', 'model.layers.16.attention_norm.weight', 'model.layers.17.attention.wqkv.weight', 'model.layers.17.attention.wo.weight', 'model.layers.17.feed_forward.w1.weight', 'model.layers.17.feed_forward.w3.weight', 'model.layers.17.feed_forward.w2.weight', 'model.layers.17.ffn_norm.weight', 'model.layers.17.attention_norm.weight', 'model.layers.18.attention.wqkv.weight', 'model.layers.18.attention.wo.weight', 'model.layers.18.feed_forward.w1.weight', 'model.layers.18.feed_forward.w3.weight', 'model.layers.18.feed_forward.w2.weight', 'model.layers.18.ffn_norm.weight', 'model.layers.18.attention_norm.weight', 'model.layers.19.attention.wqkv.weight', 'model.layers.19.attention.wo.weight', 'model.layers.19.feed_forward.w1.weight', 'model.layers.19.feed_forward.w3.weight', 'model.layers.19.feed_forward.w2.weight', 'model.layers.19.ffn_norm.weight', 'model.layers.19.attention_norm.weight', 'model.layers.20.attention.wqkv.weight', 'model.layers.20.attention.wo.weight', 'model.layers.20.feed_forward.w1.weight', 'model.layers.20.feed_forward.w3.weight', 'model.layers.20.feed_forward.w2.weight', 'model.layers.20.ffn_norm.weight', 'model.layers.20.attention_norm.weight', 'model.layers.21.attention.wqkv.weight', 'model.layers.21.attention.wo.weight', 'model.layers.21.feed_forward.w1.weight', 'model.layers.21.feed_forward.w3.weight', 'model.layers.21.feed_forward.w2.weight', 'model.layers.21.ffn_norm.weight', 'model.layers.21.attention_norm.weight', 'model.layers.22.attention.wqkv.weight', 'model.layers.22.attention.wo.weight', 'model.layers.22.feed_forward.w1.weight', 'model.layers.22.feed_forward.w3.weight', 'model.layers.22.feed_forward.w2.weight', 'model.layers.22.ffn_norm.weight', 'model.layers.22.attention_norm.weight', 'model.layers.23.attention.wqkv.weight', 'model.layers.23.attention.wo.weight', 'model.layers.23.feed_forward.w1.weight', 'model.layers.23.feed_forward.w3.weight', 'model.layers.23.feed_forward.w2.weight', 'model.layers.23.ffn_norm.weight', 'model.layers.23.attention_norm.weight', 'model.norm.weight', 'model.output.weight', 'model.fast_embeddings.weight', 'model.fast_layers.0.attention.wqkv.weight', 'model.fast_layers.0.attention.wo.weight', 'model.fast_layers.0.feed_forward.w1.weight', 'model.fast_layers.0.feed_forward.w3.weight', 'model.fast_layers.0.feed_forward.w2.weight', 'model.fast_layers.0.ffn_norm.weight', 'model.fast_layers.0.attention_norm.weight', 'model.fast_layers.1.attention.wqkv.weight', 'model.fast_layers.1.attention.wo.weight', 'model.fast_layers.1.feed_forward.w1.weight', 'model.fast_layers.1.feed_forward.w3.weight', 'model.fast_layers.1.feed_forward.w2.weight', 'model.fast_layers.1.ffn_norm.weight', 'model.fast_layers.1.attention_norm.weight', 'model.fast_layers.2.attention.wqkv.weight', 'model.fast_layers.2.attention.wo.weight', 'model.fast_layers.2.feed_forward.w1.weight', 'model.fast_layers.2.feed_forward.w3.weight', 'model.fast_layers.2.feed_forward.w2.weight', 'model.fast_layers.2.ffn_norm.weight', 'model.fast_layers.2.attention_norm.weight', 'model.fast_layers.3.attention.wqkv.weight', 'model.fast_layers.3.attention.wo.weight', 'model.fast_layers.3.feed_forward.w1.weight', 'model.fast_layers.3.feed_forward.w3.weight', 'model.fast_layers.3.feed_forward.w2.weight', 'model.fast_layers.3.ffn_norm.weight', 'model.fast_layers.3.attention_norm.weight', 'model.fast_norm.weight', 'model.fast_output.weight'], unexpected_keys=[])\nValidation DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07<00:00,  1.29it/s]\u001b[A\nEpoch 0:  38%|\u2588\u2588\u2588\u258a      | 3/8 [00:13<00:22,  0.22it/s, v_num=0, train/loss=8.380, train/top_5_accuracy=0.403, val/loss=8.650, val/top_5_accuracy=0.374]\nEpoch 0:  50%|\u2588\u2588\u2588\u2588\u2588     | 4/8 [00:15<00:15,  0.27it/s, v_num=0, train/loss=8.380, train/top_5_accuracy=0.403, val/loss=8.650, val/top_5_accuracy=0.374]\nEpoch 0:  50%|\u2588\u2588\u2588\u2588\u2588     | 4/8 [00:15<00:15,  0.27it/s, v_num=0, train/loss=8.640, train/top_5_accuracy=0.384, val/loss=8.650, val/top_5_accuracy=0.374]\nEpoch 0:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 5/8 [00:16<00:09,  0.31it/s, v_num=0, train/loss=8.640, train/top_5_accuracy=0.384, val/loss=8.650, val/top_5_accuracy=0.374]\nEpoch 0:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 5/8 [00:16<00:09,  0.31it/s, v_num=0, train/loss=8.690, train/top_5_accuracy=0.368, val/loss=8.650, val/top_5_accuracy=0.374]\nEpoch 0:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:17<00:05,  0.34it/s, v_num=0, train/loss=8.690, train/top_5_accuracy=0.368, val/loss=8.650, val/top_5_accuracy=0.374]\nEpoch 0:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:17<00:05,  0.34it/s, v_num=0, train/loss=8.810, train/top_5_accuracy=0.350, val/loss=8.650, val/top_5_accuracy=0.374]\nValidation DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:07<00:00,  1.40it/s]\u001b[A\nEpoch 0:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 6/8 [00:25<00:08,  0.23it/s, v_num=0, train/loss=8.810, train/top_5_accuracy=0.350, val/loss=8.770, val/top_5_accuracy=0.363]\nEpoch 0:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:27<00:03,  0.26it/s, v_num=0, train/loss=8.810, train/top_5_accuracy=0.350, val/loss=8.770, val/top_5_accuracy=0.363]\nEpoch 0:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 7/8 [00:27<00:03,  0.26it/s, v_num=0, train/loss=9.000, train/top_5_accuracy=0.354, val/loss=8.770, val/top_5_accuracy=0.363]\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:28<00:00,  0.28it/s, v_num=0, train/loss=9.000, train/top_5_accuracy=0.354, val/loss=8.770, val/top_5_accuracy=0.363]\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:28<00:00,  0.28it/s, v_num=0, train/loss=8.410, train/top_5_accuracy=0.381, val/loss=8.770, val/top_5_accuracy=0.363]\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:28<00:00,  0.28it/s, v_num=0, train/loss=8.410, train/top_5_accuracy=0.381, val/loss=8.770, val/top_5_accuracy=0.363]`Trainer.fit` stopped: `max_steps=8` reached.\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:28<00:00,  0.28it/s, v_num=0, train/loss=8.410, train/top_5_accuracy=0.381, val/loss=8.770, val/top_5_accuracy=0.363]\n\u2705 \u2705 Training completed successfully!\n\ud83d\udcc1 \ud83d\udcc1 Checkpoints saved in: /Users/a1/Project/OPG/checkpoints/e2e_test_resume/checkpoints\n\ud83d\udcc4    \ud83d\udcc4 step_000000003.ckpt\n\ud83d\udcc4    \ud83d\udcc4 step_000000006.ckpt\n\u2705 Resume training completed, checkpoint: step_000000006.ckpt\n\ud83d\udd0d Verifying checkpoints...\n\u2705 Initial checkpoint verified: 71.0MB\n\u2705 Resume checkpoint verified: 71.0MB\n\ud83c\udfa4 Running inference tests with trained models...\n\ud83e\uddea Testing initial checkpoint: step_000000003.ckpt\n\ud83d\udd0a Running inference: --voice RU_Google_Female_Zephyr --emotion neutral --monitor --play\n\u2705 Inference 1/3 passed (63.9s)\n\ud83d\udd0a Running inference: --voice RU_Male_Goblin_Puchkov --emotion neutral --monitor --play\n\u2705 Inference 2/3 passed (37.2s)\n\ud83d\udd0a Running inference: --voice RU_Google_Male_Achird --emotion neutral --monitor --play\n\u2705 Inference 3/3 passed (35.8s)\n\ud83e\uddea Testing resume checkpoint: step_000000006.ckpt\n\ud83d\udd0a Running inference: --voice RU_Google_Female_Zephyr --emotion neutral --monitor --play\n\u2705 Inference 1/3 passed (57.2s)\n\ud83d\udd0a Running inference: --voice RU_Male_Goblin_Puchkov --emotion neutral --monitor --play\n\u2705 Inference 2/3 passed (39.0s)\n\ud83d\udd0a Running inference: --voice RU_Google_Male_Achird --emotion neutral --monitor --play\n\u2705 Inference 3/3 passed (37.5s)\n\ud83c\udfa4 Inference testing completed: 6/6 tests passed\n\ud83d\udcca Generating test report...\n\u2705 Test report saved: /Users/a1/Project/OPG/fs-python/tests/test_report.json\n==================================================\n\ud83c\udf89 E2E Test PASSED!\n\u2705 Initial training: 5 steps\n\u2705 Resume training: 8 steps\n\u2705 Checkpoints created and verified\n\u2705 Emotional tokens working\n\u2705 Inference tests: 6/6 passed\n\u2705 Average inference time: 45.1s\n"}, "teardown": {"duration": 0.00014895800268277526, "outcome": "passed"}}]}