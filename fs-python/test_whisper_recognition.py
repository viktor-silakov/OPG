#!/usr/bin/env python3
"""
Standalone Whisper Recognition Test for Generated Audio Files
Tests audio recognition on existing wav files generated by Fish Speech
"""

import sys
import argparse
from pathlib import Path
import json
import time
from typing import List, Dict

# Try to import whisper
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("‚ùå Whisper not available. Install with: pip install openai-whisper")
    sys.exit(1)

class WhisperTester:
    """Test Whisper recognition on generated audio files"""
    
    def __init__(self, model_name: str = "turbo"):
        self.model_name = model_name
        self.model = None
        self.results = []
        
    def load_model(self):
        """Load Whisper model"""
        print(f"üéß Loading Whisper {self.model_name} model...")
        try:
            self.model = whisper.load_model(self.model_name)
            print("‚úÖ Whisper model loaded successfully")
            return True
        except Exception as e:
            print(f"‚ùå Failed to load Whisper model: {e}")
            return False
    
    def transcribe_file(self, audio_file: Path, expected_text: str = None) -> Dict:
        """Transcribe a single audio file"""
        if not self.model:
            return {'success': False, 'error': 'Model not loaded'}
        
        print(f"\nüé§ Transcribing: {audio_file.name}")
        
        try:
            start_time = time.time()
            result = self.model.transcribe(str(audio_file), language="ru")
            duration = time.time() - start_time
            
            transcribed_text = result["text"].strip()
            
            print(f"üìù Transcribed: '{transcribed_text}'")
            print(f"‚è±Ô∏è Duration: {duration:.2f}s")
            
            # Calculate similarity if expected text provided
            similarity_data = {}
            if expected_text:
                print(f"\nüìù TEXT COMPARISON for {audio_file.name}:")
                print(f"üéØ Original text:    '{expected_text}'")
                print(f"üé§ Whisper result:   '{transcribed_text}'")
                
                similarity_data = self._calculate_similarity(expected_text, transcribed_text)
                print(f"üìä Similarity: {similarity_data['similarity_ratio']:.3f}")
                print(f"üìä Word similarity: {similarity_data['word_similarity']:.3f}")
                print(f"üî§ Char accuracy: {similarity_data['char_accuracy']:.3f}")
                
                threshold = 0.6  # Same threshold as in main test
                passes_threshold = similarity_data['similarity_ratio'] >= threshold
                print(f"‚úÖ Passes threshold ({threshold}): {'YES' if passes_threshold else 'NO'}")
            
            result_data = {
                'success': True,
                'file': str(audio_file),
                'transcribed_text': transcribed_text,
                'expected_text': expected_text,
                'duration_sec': round(duration, 2),
                'language': result.get("language", "unknown"),
                'segments_count': len(result.get("segments", [])),
                **similarity_data
            }
            
            self.results.append(result_data)
            return result_data
            
        except Exception as e:
            print(f"‚ùå Transcription failed: {e}")
            error_result = {
                'success': False,
                'file': str(audio_file),
                'error': str(e),
                'expected_text': expected_text
            }
            self.results.append(error_result)
            return error_result
    
    def _calculate_similarity(self, original_text: str, transcribed_text: str) -> Dict:
        """Calculate similarity between original and transcribed text"""
        import difflib
        import re
        
        # Clean texts for comparison
        original_clean = self._clean_text(original_text)
        transcribed_clean = self._clean_text(transcribed_text)
        
        # Calculate different similarity metrics
        sequence_matcher = difflib.SequenceMatcher(None, original_clean, transcribed_clean)
        similarity_ratio = sequence_matcher.ratio()
        
        # Word-level comparison
        original_words = original_clean.split()
        transcribed_words = transcribed_clean.split()
        
        word_matcher = difflib.SequenceMatcher(None, original_words, transcribed_words)
        word_similarity = word_matcher.ratio()
        
        # Character accuracy (simple)
        char_accuracy = sum(1 for a, b in zip(original_clean, transcribed_clean) if a == b) / max(len(original_clean), len(transcribed_clean), 1)
        
        return {
            'similarity_ratio': round(similarity_ratio, 3),
            'word_similarity': round(word_similarity, 3),
            'char_accuracy': round(char_accuracy, 3),
            'original_clean': original_clean,
            'transcribed_clean': transcribed_clean
        }
    
    def _clean_text(self, text: str) -> str:
        """Clean text for comparison"""
        import re
        
        # Remove emotional tokens like (joyful), (sad), etc.
        text = re.sub(r'\([^)]+\)', '', text)
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        # Convert to lowercase for comparison
        text = text.lower()
        
        # Remove punctuation for more lenient comparison
        text = re.sub(r'[^\w\s]', '', text)
        
        return text.strip()
    
    def test_directory(self, directory: Path, expected_texts: Dict[str, str] = None):
        """Test all wav files in a directory"""
        print(f"üîç Scanning directory: {directory}")
        
        wav_files = list(directory.glob("*.wav"))
        if not wav_files:
            print("‚ö†Ô∏è No wav files found in directory")
            return
        
        print(f"üìÅ Found {len(wav_files)} wav files")
        
        for wav_file in sorted(wav_files):
            expected_text = None
            if expected_texts:
                expected_text = expected_texts.get(wav_file.name)
            
            self.transcribe_file(wav_file, expected_text)
    
    def generate_report(self, output_file: Path = None):
        """Generate test report"""
        if not self.results:
            print("‚ö†Ô∏è No results to report")
            return
        
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r['success'])
        
        # Calculate statistics for successful tests with expected text
        similarity_results = [r for r in self.results if r['success'] and 'similarity_ratio' in r]
        avg_similarity = sum(r['similarity_ratio'] for r in similarity_results) / len(similarity_results) if similarity_results else 0
        
        report = {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'whisper_model': self.model_name,
            'summary': {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'success_rate': round(successful_tests / total_tests * 100, 1),
                'tests_with_similarity': len(similarity_results),
                'average_similarity': round(avg_similarity, 3) if similarity_results else None
            },
            'detailed_results': self.results
        }
        
        # Print summary
        print("\n" + "="*60)
        print("üìä WHISPER RECOGNITION TEST RESULTS")
        print("="*60)
        print(f"‚úÖ Successful tests: {successful_tests}/{total_tests} ({report['summary']['success_rate']}%)")
        
        if similarity_results:
            print(f"üéØ Average similarity: {avg_similarity:.3f}")
            high_similarity = sum(1 for r in similarity_results if r['similarity_ratio'] >= 0.7)
            print(f"üéñÔ∏è High similarity (‚â•0.7): {high_similarity}/{len(similarity_results)}")
        
        if successful_tests > 0:
            avg_duration = sum(r.get('duration_sec', 0) for r in self.results if r['success']) / successful_tests
            print(f"‚è±Ô∏è Average transcription time: {avg_duration:.2f}s")
        
        # Save report if requested
        if output_file:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"üíæ Report saved: {output_file}")

def main():
    parser = argparse.ArgumentParser(description="Test Whisper recognition on generated audio files")
    parser.add_argument("input_path", help="Path to wav file or directory containing wav files")
    parser.add_argument("--model", default="turbo", help="Whisper model to use (default: turbo)")
    parser.add_argument("--expected", help="JSON file with expected texts for comparison")
    parser.add_argument("--output", help="Output file for test report")
    
    args = parser.parse_args()
    
    input_path = Path(args.input_path)
    if not input_path.exists():
        print(f"‚ùå Input path does not exist: {input_path}")
        sys.exit(1)
    
    # Load expected texts if provided
    expected_texts = {}
    if args.expected:
        expected_file = Path(args.expected)
        if expected_file.exists():
            with open(expected_file, 'r', encoding='utf-8') as f:
                expected_texts = json.load(f)
            print(f"üìã Loaded expected texts for {len(expected_texts)} files")
    
    # Initialize tester
    tester = WhisperTester(args.model)
    if not tester.load_model():
        sys.exit(1)
    
    # Run tests
    if input_path.is_file() and input_path.suffix.lower() == '.wav':
        # Single file
        expected_text = expected_texts.get(input_path.name)
        tester.transcribe_file(input_path, expected_text)
    elif input_path.is_dir():
        # Directory
        tester.test_directory(input_path, expected_texts)
    else:
        print(f"‚ùå Invalid input: {input_path} (must be wav file or directory)")
        sys.exit(1)
    
    # Generate report
    output_file = Path(args.output) if args.output else None
    tester.generate_report(output_file)

if __name__ == "__main__":
    main() 